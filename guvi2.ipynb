{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjTqic8LQF_y"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "kf_hv6EHQNB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "id": "tFV-de6jQP1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id1=\"dreamlike-art/dreamlike-diffusion-1.0\"\n",
        "model_id2=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id1, use_safetensors=True)\n",
        "\n",
        "# Check if CUDA is available and move the pipeline accordingly\n",
        "if torch.cuda.is_available():\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "else:\n",
        "    pipe = pipe.to(\"cpu\")\n",
        "    print(\"CUDA not available, using CPU.\")"
      ],
      "metadata": {
        "id": "7BBjLN80QRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"dreamlikeart,a grungy woment with rinbow hair, travelling between dimensions,dynamic pose,happy,soft extreme bokeh,danity figure,long hair straight down,torn kawaii shirt and baggy jeans\"\"\""
      ],
      "metadata": {
        "id": "cRZs20RpQZv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=pipe(prompt).images[0]"
      ],
      "metadata": {
        "id": "zblMYmwxQbtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "u1uCvpG7Qk4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[PROMPT]:\",image)\n",
        "plt.imshow(image);\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "bPomREemQqHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(pipe,prompt,params):\n",
        "  img = pipe(prompt,**params).images\n",
        "  num_images=len(img)\n",
        "  if num_images>1:\n",
        "    fig,ax=plt.subplots(nrows=1,ncols=num_images)\n",
        "    for i in range(num_images):\n",
        "      ax[i].imshow(img[i])\n",
        "      ax[i].axis('off')\n",
        "  else:\n",
        "    fig=plt.figure()\n",
        "    plt.imshow(img[0]);\n",
        "    plt.axis('off');\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "_JYTmJ2UQyDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1=\"\"\"one boy lying on the bed and watching movie in phone\"\"\""
      ],
      "metadata": {
        "id": "7RzZYEJpRDpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={}"
      ],
      "metadata": {
        "id": "CIyJBpp1RJOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_image(pipe,text1,params)\n"
      ],
      "metadata": {
        "id": "4q1APQ7LRKwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'num_inference_steps':100}\n",
        "generate_image(pipe,text1,params)"
      ],
      "metadata": {
        "id": "eYPM6CtuRNHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={'num_inference_steps':100,'width':512,'height': int(1.5*640)}\n",
        "generate_image(pipe,prompt,params)"
      ],
      "metadata": {
        "id": "SmJmshreRPbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params ={'num_inference_steps':100, 'num_images_per_prompt':2}\n",
        "generate_image(pipe,prompt,params)"
      ],
      "metadata": {
        "id": "xV6mD_EHRR_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={'num_interface_steps':100,'num_images_per_prompt':2,'negative_prompt':'ugly,distorted,low_quality'}\n",
        "generate_image(pipe,prompt,params)"
      ],
      "metadata": {
        "id": "Y8FZEMOeRWfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit transformers diffusers accelerate\n",
        "\n",
        "with open('guvi.py', 'w') as f:\n",
        "  f.write('''\n",
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length (tokens)\", min_value=100, max_value=600, value=300, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with the actual paths to your extracted model directories\n",
        "LOCAL_TEXT_MODEL_PATH = \"./local_text_model\" # Example: Adjust this path as needed\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"./local_image_pipeline\" # Example: Adjust this path as needed\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer (from local) ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        model.to(\"cuda\")\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline (from local) ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        pipe.to(\"cuda\")\n",
        "    else:\n",
        "        st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "    return pipe\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "  with st.spinner(\"Generating your story and images...\"):\n",
        "    # --- Text Generation ---\n",
        "    # Use the loaded local model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "    generated_ids = text_model.generate(\n",
        "        **inputs,\n",
        "        max_length=length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.85,\n",
        "        top_k=50,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Basic attempt to split into segments (can be improved)\n",
        "    segments = generated_text.split('.') # Split by sentence for now\n",
        "    story_segments = [{\"text\": segment.strip() + '.', \"prompt_for_image\": f\"Illustration for a story scene: {segment.strip()}\"} for segment in segments if segment.strip()]\n",
        "\n",
        "    st.subheader(\"Generated Story\")\n",
        "\n",
        "    # --- Display Story Segments and Generate Images ---\n",
        "    for i, segment in enumerate(story_segments):\n",
        "        st.markdown(f\"**Scene {i+1}:**\")\n",
        "        st.write(segment[\"text\"])\n",
        "\n",
        "        # --- Image Generation ---\n",
        "        if segment.get(\"prompt_for_image\"):\n",
        "            try:\n",
        "                image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                st.image(image, caption=f\"Scene {i+1} Illustration\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating image for Scene {i+1}: {e}\")\n",
        "        else:\n",
        "            st.info(f\"No image prompt generated for Scene {i+1}.\")\n",
        "\n",
        "\n",
        "    # --- Optional Export/Enhancements Section (Placeholders) ---\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Options\")\n",
        "    # Placeholders for Export and Optional Enhancements\n",
        "    # st.button(\"Export as PDF\")\n",
        "    # st.button(\"Export as Storybook\")\n",
        "    # st.button(\"Export as Image Gallery\")\n",
        "    # st.checkbox(\"Enable Narration\")\n",
        "    # st.checkbox(\"Add Background Music\")\n",
        "  ''')"
      ],
      "metadata": {
        "id": "vnm-0rijR6Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('guvi.py', 'w') as f:\n",
        "  f.write('''\n",
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length per part (tokens)\", min_value=100, max_value=300, value=200, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with the actual paths to your extracted model directories\n",
        "LOCAL_TEXT_MODEL_PATH = \"./local_text_model\" # Example: Adjust this path as needed\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"./local_image_pipeline\" # Example: Adjust this path as needed\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer (from local) ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        model.to(\"cuda\")\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline (from local) ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        pipe.to(\"cuda\")\n",
        "    else:\n",
        "        st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "    return pipe\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "  with st.spinner(\"Generating your story and images...\"):\n",
        "    story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"]\n",
        "    full_story = \"\"\n",
        "    story_segments_for_images = []\n",
        "\n",
        "    for part in story_parts:\n",
        "        part_prompt = f\"Write the {part} for a {genre} story with a {tone} tone about: {prompt}\"\n",
        "        inputs = tokenizer(part_prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "        generated_ids = text_model.generate(\n",
        "            **inputs,\n",
        "            max_length=len(tokenizer(part_prompt).input_ids) + length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.85,\n",
        "            top_k=50,\n",
        "            top_p=0.92,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Simple transition\n",
        "        transition = \"\"\n",
        "        if part == \"Introduction\":\n",
        "            transition = \"\\\\n\\\\n\"\n",
        "        elif part == \"Conflict\":\n",
        "            transition = \"\\\\n\\\\nHowever, \"\n",
        "        elif part == \"Climax\":\n",
        "            transition = \"\\\\n\\\\nSuddenly, \"\n",
        "        elif part == \"Resolution\":\n",
        "            transition = \"\\\\n\\\\nIn the end, \"\n",
        "\n",
        "        full_story += generated_text + transition\n",
        "        story_segments_for_images.append({\"text\": generated_text.strip(), \"prompt_for_image\": f\"Illustration for the {part} of a story: {generated_text.strip()[:100]}\"}) # Use first 100 chars for image prompt\n",
        "\n",
        "\n",
        "    st.subheader(\"Generated Story\")\n",
        "\n",
        "    # --- Display Story Segments and Generate Images ---\n",
        "    for i, segment in enumerate(story_segments_for_images):\n",
        "        st.markdown(f\"**{story_parts[i]}:**\")\n",
        "        st.write(segment[\"text\"])\n",
        "\n",
        "        # --- Image Generation ---\n",
        "        if segment.get(\"prompt_for_image\"):\n",
        "            try:\n",
        "                image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                st.image(image, caption=f\"{story_parts[i]} Illustration\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating image for {story_parts[i]}: {e}\")\n",
        "        else:\n",
        "            st.info(f\"No image prompt generated for {story_parts[i]}.\")\n",
        "\n",
        "\n",
        "    # --- Optional Export/Enhancements Section (Placeholders) ---\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Options\")\n",
        "    # Placeholders for Export and Optional Enhancements\n",
        "    # st.button(\"Export as PDF\")\n",
        "    # st.button(\"Export as Storybook\")\n",
        "    # st.button(\"Export as Image Gallery\")\n",
        "    # st.checkbox(\"Enable Narration\")\n",
        "    # st.checkbox(\"Add Background Music\")\n",
        "  ''')"
      ],
      "metadata": {
        "id": "auJABOfgSDj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('guvi.py', 'w') as f:\n",
        "  f.write('''\n",
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import re # Import regular expression library\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length per part (tokens)\", min_value=100, max_value=300, value=200, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with the actual paths to your extracted model directories\n",
        "LOCAL_TEXT_MODEL_PATH = \"./local_text_model\" # Example: Adjust this path as needed\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"./local_image_pipeline\" # Example: Adjust this path as needed\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer (from local) ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        model.to(\"cuda\")\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline (from local) ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True)\n",
        "    # Move to GPU if available (optional, depends on local setup)\n",
        "    if torch.cuda.is_available():\n",
        "        pipe.to(\"cuda\")\n",
        "    else:\n",
        "        st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "    return pipe\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "# --- Function to generate image prompt based on story segment content ---\n",
        "def generate_image_prompt(segment_text, part, genre, tone):\n",
        "    # Simple keyword extraction (can be enhanced with more sophisticated NLP)\n",
        "    keywords = re.findall(r'\\b\\w{4,}\\b', segment_text) # Extract words with 4 or more characters\n",
        "    # Filter out common words and keep unique ones\n",
        "    common_words = set([\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"is\", \"it\", \"that\", \"on\", \"with\", \"for\", \"by\", \"this\", \"about\", \"are\", \"from\", \"was\", \"were\"])\n",
        "    filtered_keywords = [word for word in keywords if word.lower() not in common_words]\n",
        "    unique_keywords = list(set(filtered_keywords))\n",
        "\n",
        "    # Construct the prompt\n",
        "    image_prompt = f\"Illustration for the {part} of a {genre} story with a {tone} tone. Focus on: {', '.join(unique_keywords[:10])}. Scene description: {segment_text[:200]}...\" # Use up to 10 unique keywords and a snippet of the text\n",
        "\n",
        "    return image_prompt\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "  with st.spinner(\"Generating your story and images...\"):\n",
        "    story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"]\n",
        "    full_story = \"\"\n",
        "    story_segments_for_images = []\n",
        "\n",
        "    for part in story_parts:\n",
        "        part_prompt = f\"Write the {part} for a {genre} story with a {tone} tone about: {prompt}\"\n",
        "        inputs = tokenizer(part_prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {key: value.to(\"cuda\") for key in inputs.keys()}\n",
        "\n",
        "\n",
        "        generated_ids = text_model.generate(\n",
        "            **inputs,\n",
        "            max_length=len(tokenizer(part_prompt).input_ids) + length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.85,\n",
        "            top_k=50,\n",
        "            top_p=0.92,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Simple transition\n",
        "        transition = \"\"\n",
        "        if part == \"Introduction\":\n",
        "            transition = \"\\\\n\\\\n\"\n",
        "        elif part == \"Conflict\":\n",
        "            transition = \"\\\\n\\\\nHowever, \"\n",
        "        elif part == \"Climax\":\n",
        "            transition = \"\\\\n\\\\nSuddenly, \"\n",
        "        elif part == \"Resolution\":\n",
        "            transition = \"\\\\n\\\\nIn the end, \"\n",
        "\n",
        "        full_story += generated_text + transition\n",
        "\n",
        "        # Generate a more descriptive image prompt\n",
        "        image_prompt = generate_image_prompt(generated_text.strip(), part, genre, tone)\n",
        "        story_segments_for_images.append({\"text\": generated_text.strip(), \"prompt_for_image\": image_prompt})\n",
        "\n",
        "\n",
        "    st.subheader(\"Generated Story\")\n",
        "\n",
        "    # --- Display Story Segments and Generate Images ---\n",
        "    for i, segment in enumerate(story_segments_for_images):\n",
        "        st.markdown(f\"**{story_parts[i]}:**\")\n",
        "        st.write(segment[\"text\"])\n",
        "\n",
        "        # --- Image Generation ---\n",
        "        if segment.get(\"prompt_for_image\"):\n",
        "            try:\n",
        "                image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                st.image(image, caption=f\"{story_parts[i]} Illustration\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating image for {story_parts[i]}: {e}\")\n",
        "        else:\n",
        "            st.info(f\"No image prompt generated for {story_parts[i]}.\")\n",
        "\n",
        "\n",
        "    # --- Optional Export/Enhancements Section (Placeholders) ---\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Options\")\n",
        "    # Placeholders for Export and Optional Enhancements\n",
        "    # st.button(\"Export as PDF\")\n",
        "    # st.button(\"Export as Storybook\")\n",
        "    # st.button(\"Export as Image Gallery\")\n",
        "    # st.checkbox(\"Enable Narration\")\n",
        "    # st.checkbox(\"Add Background Music\")\n",
        "  ''')"
      ],
      "metadata": {
        "id": "2nqzx9r3SPj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "o1IPHLHoSQaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "# import reportlab # Uncomment if implementing PDF export\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length per part (tokens)\", min_value=100, max_value=300, value=200, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with actual valid Hugging Face model IDs or local paths to models\n",
        "# formatted correctly for AutoTokenizer.from_pretrained and StableDiffusionPipeline.from_pretrained\n",
        "LOCAL_TEXT_MODEL_PATH = \"gpt2\" # Example: Using a public model for demonstration\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"runwayml/stable-diffusion-v1-5\" # Example: Using a public model for demonstration\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to(\"cuda\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading text model: {e}. Please ensure LOCAL_TEXT_MODEL_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None, None\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            pipe.to(\"cuda\")\n",
        "        else:\n",
        "            st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading image pipeline: {e}. Please ensure LOCAL_IMAGE_PIPELINE_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "\n",
        "# --- Function to generate image prompt based on story segment content ---\n",
        "def generate_image_prompt(segment_text, part, genre, tone):\n",
        "    # Simple keyword extraction (can be enhanced with more sophisticated NLP)\n",
        "    keywords = re.findall(r'\\b\\w{4,}\\b', segment_text) # Extract words with 4 or more characters\n",
        "    # Filter out common words and keep unique ones\n",
        "    common_words = set([\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"is\", \"it\", \"that\", \"on\", \"with\", \"for\", \"by\", \"this\", \"about\", \"are\", \"from\", \"was\", \"were\"])\n",
        "    filtered_keywords = [word for word in keywords if word.lower() not in common_words]\n",
        "    unique_keywords = list(set(filtered_keywords))\n",
        "\n",
        "    # Construct the prompt\n",
        "    image_prompt = f\"Illustration for the {part} of a {genre} story with a {tone} tone. Focus on: {', '.join(unique_keywords[:10])}. Scene description: {segment_text[:200]}...\" # Use up to 10 unique keywords and a snippet of the text\n",
        "\n",
        "    return image_prompt\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "    if tokenizer is not None and text_model is not None and image_pipe is not None:\n",
        "        with st.spinner(\"Generating your story and images...\"):\n",
        "            story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"]\n",
        "            full_story = \"\"\n",
        "            story_segments_for_images = []\n",
        "\n",
        "            for part in story_parts:\n",
        "                part_prompt = f\"Write the {part} for a {genre} story with a {tone} tone about: {prompt}\"\n",
        "                inputs = tokenizer(part_prompt, return_tensors=\"pt\")\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {key: value.to(\"cuda\") for key in inputs.keys()}\n",
        "\n",
        "                generated_ids = text_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=len(tokenizer(part_prompt).input_ids) + length,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.85,\n",
        "                    top_k=50,\n",
        "                    top_p=0.92,\n",
        "                    repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                # Simple transition\n",
        "                transition = \"\"\n",
        "                if part == \"Introduction\":\n",
        "                    transition = \"\\\\n\\\\n\"\n",
        "                elif part == \"Conflict\":\n",
        "                    transition = \"\\\\n\\\\nHowever, \"\n",
        "                elif part == \"Climax\":\n",
        "                    transition = \"\\\\n\\\\nSuddenly, \"\n",
        "                elif part == \"Resolution\":\n",
        "                    transition = \"\\\\n\\\\nIn the end, \"\n",
        "\n",
        "                full_story += generated_text + transition\n",
        "\n",
        "                # Generate a more descriptive image prompt\n",
        "                image_prompt = generate_image_prompt(generated_text.strip(), part, genre, tone)\n",
        "                story_segments_for_images.append({\"text\": generated_text.strip(), \"prompt_for_image\": image_prompt, \"image\": None}) # Add a placeholder for the generated image\n",
        "\n",
        "\n",
        "            st.subheader(\"Generated Story\")\n",
        "\n",
        "            # --- Display Story Segments and Generate Images ---\n",
        "            for i, segment in enumerate(story_segments_for_images):\n",
        "                st.markdown(f\"**{story_parts[i]}:**\")\n",
        "                st.write(segment[\"text\"])\n",
        "\n",
        "                # --- Image Generation ---\n",
        "                if segment.get(\"prompt_for_image\"):\n",
        "                    try:\n",
        "                        image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                        segment[\"image\"] = image # Store the generated image\n",
        "                        st.image(image, caption=f\"{story_parts[i]} Illustration\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error generating image for {story_parts[i]}: {e}\")\n",
        "                else:\n",
        "                    st.info(f\"No image prompt generated for {story_parts[i]}.\")\n",
        "\n",
        "\n",
        "            # --- Optional Export/Enhancements Section ---\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Options\")\n",
        "\n",
        "            # Store story data in session state for export\n",
        "            st.session_state['story_data'] = story_segments_for_images\n",
        "            st.session_state['full_story_text'] = full_story\n",
        "    else:\n",
        "        st.error(\"Model loading failed. Please check your model paths.\")\n",
        "\n",
        "\n",
        "# --- Export Functionality ---\n",
        "if 'story_data' in st.session_state and st.session_state['story_data']:\n",
        "    story_segments_for_images = st.session_state['story_data']\n",
        "    full_story_text = st.session_state['full_story_text']\n",
        "    story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"] # Define story_parts again for the export section\n",
        "\n",
        "    # Export as Text File\n",
        "    st.download_button(\n",
        "        label=\"Export as Text File\",\n",
        "        data=full_story_text,\n",
        "        file_name=\"story.txt\",\n",
        "        mime=\"text/plain\"\n",
        "    )\n",
        "\n",
        "    # Export as Image Gallery (ZIP file of images)\n",
        "    if any(segment[\"image\"] is not None for segment in story_segments_for_images):\n",
        "        with io.BytesIO() as archive:\n",
        "            with zipfile.ZipFile(archive, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for i, segment in enumerate(story_segments_for_images):\n",
        "                    if segment[\"image\"]:\n",
        "                        img_byte_arr = io.BytesIO()\n",
        "                        segment[\"image\"].save(img_byte_arr, format='PNG')\n",
        "                        zipf.writestr(f'scene_{i+1}_{story_parts[i].lower().replace(\" \", \"_\")}.png', img_byte_arr.getvalue())\n",
        "            st.download_button(\n",
        "                label=\"Export as Image Gallery (ZIP)\",\n",
        "                data=archive.getvalue(),\n",
        "                file_name=\"story_images.zip\",\n",
        "                mime=\"application/zip\"\n",
        "            )\n",
        "\n",
        "    # Placeholder for Export as PDF (requires additional libraries like reportlab)\n",
        "    # st.markdown(\"*(PDF export requires additional libraries and implementation)*\")\n",
        "\n",
        "    # Placeholder for Export as Storybook (requires custom formatting/HTML generation)\n",
        "    # st.markdown(\"*(Storybook export requires custom formatting and implementation)*\")"
      ],
      "metadata": {
        "id": "c8ZzNnO6SYiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "wbEqCBPkSp5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7HQzkidSySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f1539ee"
      },
      "source": [
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "# import reportlab # Uncomment if implementing PDF export\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length per part (tokens)\", min_value=100, max_value=300, value=200, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with actual valid Hugging Face model IDs or local paths to models\n",
        "# formatted correctly for AutoTokenizer.from_pretrained and StableDiffusionPipeline.from_pretrained\n",
        "LOCAL_TEXT_MODEL_PATH = \"gpt2\" # Example: Using a public model for demonstration\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"runwayml/stable-diffusion-v1-5\" # Example: Using a public model for demonstration\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to(\"cuda\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading text model: {e}. Please ensure LOCAL_TEXT_MODEL_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None, None\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            pipe.to(\"cuda\")\n",
        "        else:\n",
        "            st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading image pipeline: {e}. Please ensure LOCAL_IMAGE_PIPELINE_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "\n",
        "# --- Function to generate image prompt based on story segment content ---\n",
        "def generate_image_prompt(segment_text, part, genre, tone):\n",
        "    # Simple keyword extraction (can be enhanced with more sophisticated NLP)\n",
        "    keywords = re.findall(r'\\b\\w{4,}\\b', segment_text) # Extract words with 4 or more characters\n",
        "    # Filter out common words and keep unique ones\n",
        "    common_words = set([\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"is\", \"it\", \"that\", \"on\", \"with\", \"for\", \"by\", \"this\", \"about\", \"are\", \"from\", \"was\", \"were\"])\n",
        "    filtered_keywords = [word for word in keywords if word.lower() not in common_words]\n",
        "    unique_keywords = list(set(filtered_keywords))\n",
        "\n",
        "    # Construct the prompt\n",
        "    image_prompt = f\"Illustration for the {part} of a {genre} story with a {tone} tone. Focus on: {', '.join(unique_keywords[:10])}. Scene description: {segment_text[:200]}...\" # Use up to 10 unique keywords and a snippet of the text\n",
        "\n",
        "    return image_prompt\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "    if tokenizer is not None and text_model is not None and image_pipe is not None:\n",
        "        with st.spinner(\"Generating your story and images...\"):\n",
        "            story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"]\n",
        "            full_story = \"\"\n",
        "            story_segments_for_images = []\n",
        "\n",
        "            for part in story_parts:\n",
        "                part_prompt = f\"Write the {part} for a {genre} story with a {tone} tone about: {prompt}\"\n",
        "                inputs = tokenizer(part_prompt, return_tensors=\"pt\")\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {key: value.to(\"cuda\") for key in inputs.keys()}\n",
        "\n",
        "                generated_ids = text_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=len(tokenizer(part_prompt).input_ids) + length,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.85,\n",
        "                    top_k=50,\n",
        "                    top_p=0.92,\n",
        "                    repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                # Simple transition\n",
        "                transition = \"\"\n",
        "                if part == \"Introduction\":\n",
        "                    transition = \"\\\\n\\\\n\"\n",
        "                elif part == \"Conflict\":\n",
        "                    transition = \"\\\\n\\\\nHowever, \"\n",
        "                elif part == \"Climax\":\n",
        "                    transition = \"\\\\n\\\\nSuddenly, \"\n",
        "                elif part == \"Resolution\":\n",
        "                    transition = \"\\\\n\\\\nIn the end, \"\n",
        "\n",
        "                full_story += generated_text + transition\n",
        "\n",
        "                # Generate a more descriptive image prompt\n",
        "                image_prompt = generate_image_prompt(generated_text.strip(), part, genre, tone)\n",
        "                story_segments_for_images.append({\"text\": generated_text.strip(), \"prompt_for_image\": image_prompt, \"image\": None}) # Add a placeholder for the generated image\n",
        "\n",
        "\n",
        "            st.subheader(\"Generated Story\")\n",
        "\n",
        "            # --- Display Story Segments and Generate Images ---\n",
        "            for i, segment in enumerate(story_segments_for_images):\n",
        "                st.markdown(f\"**{story_parts[i]}:**\")\n",
        "                st.write(segment[\"text\"])\n",
        "\n",
        "                # --- Image Generation ---\n",
        "                if segment.get(\"prompt_for_image\"):\n",
        "                    try:\n",
        "                        image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                        segment[\"image\"] = image # Store the generated image\n",
        "                        st.image(image, caption=f\"{story_parts[i]} Illustration\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error generating image for {story_parts[i]}: {e}\")\n",
        "                else:\n",
        "                    st.info(f\"No image prompt generated for {story_parts[i]}.\")\n",
        "\n",
        "\n",
        "            # --- Optional Export/Enhancements Section ---\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Options\")\n",
        "\n",
        "            # Store story data in session state for export\n",
        "            st.session_state['story_data'] = story_segments_for_images\n",
        "            st.session_state['full_story_text'] = full_story\n",
        "    else:\n",
        "        st.error(\"Model loading failed. Please check your model paths.\")\n",
        "\n",
        "\n",
        "# --- Export Functionality ---\n",
        "if 'story_data' in st.session_state and st.session_state['story_data']:\n",
        "    story_segments_for_images = st.session_state['story_data']\n",
        "    full_story_text = st.session_state['full_story_text']\n",
        "    story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"] # Define story_parts again for the export section\n",
        "\n",
        "    # Export as Text File\n",
        "    st.download_button(\n",
        "        label=\"Export as Text File\",\n",
        "        data=full_story_text,\n",
        "        file_name=\"story.txt\",\n",
        "        mime=\"text/plain\"\n",
        "    )\n",
        "\n",
        "    # Export as Image Gallery (ZIP file of images)\n",
        "    if any(segment[\"image\"] is not None for segment in story_segments_for_images):\n",
        "        with io.BytesIO() as archive:\n",
        "            with zipfile.ZipFile(archive, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for i, segment in enumerate(story_segments_for_images):\n",
        "                    if segment[\"image\"]:\n",
        "                        img_byte_arr = io.BytesIO()\n",
        "                        segment[\"image\"].save(img_byte_arr, format='PNG')\n",
        "                        zipf.writestr(f'scene_{i+1}_{story_parts[i].lower().replace(\" \", \"_\")}.png', img_byte_arr.getvalue())\n",
        "            st.download_button(\n",
        "                label=\"Export as Image Gallery (ZIP)\",\n",
        "                data=archive.getvalue(),\n",
        "                file_name=\"story_images.zip\",\n",
        "                mime=\"application/zip\"\n",
        "            )\n",
        "\n",
        "    # Placeholder for Export as PDF (requires additional libraries like reportlab)\n",
        "    # st.markdown(\"*(PDF export requires additional libraries and implementation)*\")\n",
        "\n",
        "    # Placeholder for Export as Storybook (requires custom formatting/HTML generation)\n",
        "    # st.markdown(\"*(Storybook export requires custom formatting and implementation)*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile guvi.py\n",
        "# Set environment variable to potentially help with CUDA memory fragmentation\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "print(\"Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "# import reportlab # Uncomment if implementing PDF export\n",
        "\n",
        "st.set_page_config(page_title=\"AI Story Generator with Image\", page_icon=\"\")\n",
        "st.title(\"ðŸ“™ AI STORY GENERATOR WITH IMAGE\")\n",
        "st.markdown(\"Type a creative prompt, choose your settings, and let the AI generate a multi-part story with images!\")\n",
        "\n",
        "# --- User Input Section ---\n",
        "st.subheader(\"Story Idea and Settings\")\n",
        "prompt = st.text_input(\"ðŸ–‹ Enter the core story idea:\", \"A young girl finds a secret door in her grandmother's attic.\")\n",
        "\n",
        "# Optional settings - these can be used later to influence text generation\n",
        "genre = st.selectbox(\"Select Genre:\", [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Comedy\", \"Other\"])\n",
        "tone = st.selectbox(\"Select Tone:\", [\"Lighthearted\", \"Dark\", \"Epic\", \"Humorous\", \"Serious\"])\n",
        "audience = st.selectbox(\"Target Audience:\", [\"Kids\", \"Teens\", \"Adults\", \"All\"])\n",
        "\n",
        "length = st.slider(\"ðŸ–‹ Max Story Length per part (tokens)\", min_value=100, max_value=300, value=200, step=20) # Increased max length for multi-part story\n",
        "\n",
        "# --- Local Model Paths ---\n",
        "# IMPORTANT: Replace these paths with actual valid Hugging Face model IDs or local paths to models\n",
        "# formatted correctly for AutoTokenizer.from_pretrained and StableDiffusionPipeline.from_pretrained\n",
        "LOCAL_TEXT_MODEL_PATH = \"gpt2\" # Example: Using a public model for demonstration\n",
        "# Consider using a smaller model or torch_dtype=torch.float16 if you encounter CUDA memory issues\n",
        "LOCAL_IMAGE_PIPELINE_PATH = \"runwayml/stable-diffusion-v1-5\" # Example: Using a public model for demonstration\n",
        "\n",
        "\n",
        "# --- Load Text Generation Model and Tokenizer ---\n",
        "@st.cache_resource\n",
        "def load_text_model(model_path):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            model.to(\"cuda\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading text model: {e}. Please ensure LOCAL_TEXT_MODEL_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None, None\n",
        "\n",
        "tokenizer, text_model = load_text_model(LOCAL_TEXT_MODEL_PATH)\n",
        "\n",
        "\n",
        "# --- Load Image Generation Pipeline ---\n",
        "@st.cache_resource\n",
        "def load_image_pipeline(pipeline_path):\n",
        "    try:\n",
        "        # Use torch_dtype=torch.float16 to potentially reduce memory usage\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(pipeline_path, use_safetensors=True, torch_dtype=torch.float16)\n",
        "        # Move to GPU if available (optional, depends on local setup)\n",
        "        if torch.cuda.is_available():\n",
        "            pipe.to(\"cuda\")\n",
        "        else:\n",
        "            st.warning(\"CUDA not available, using CPU. Image generation may be slow.\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading image pipeline: {e}. Please ensure LOCAL_IMAGE_PIPELINE_PATH is a valid Hugging Face model ID or a correctly formatted local path.\")\n",
        "        return None\n",
        "\n",
        "image_pipe = load_image_pipeline(LOCAL_IMAGE_PIPELINE_PATH)\n",
        "\n",
        "\n",
        "# --- Function to generate image prompt based on story segment content ---\n",
        "def generate_image_prompt(segment_text, part, genre, tone):\n",
        "    # Simple keyword extraction (can be enhanced with more sophisticated NLP)\n",
        "    keywords = re.findall(r'\\b\\w{4,}\\b', segment_text) # Extract words with 4 or more characters\n",
        "    # Filter out common words and keep unique ones\n",
        "    common_words = set([\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"is\", \"it\", \"that\", \"on\", \"with\", \"for\", \"by\", \"this\", \"about\", \"are\", \"from\", \"was\", \"were\"])\n",
        "    filtered_keywords = [word for word in keywords if word.lower() not in common_words]\n",
        "    unique_keywords = list(set(filtered_keywords))\n",
        "\n",
        "    # Construct the prompt - make it more descriptive for better image generation and add emphasis on color\n",
        "    image_prompt = f\"Vibrant and detailed illustration for the {part} of a {genre} story with a {tone} tone. Scene focuses on: {', '.join(unique_keywords[:15])}. Full scene description: {segment_text[:300]}. Emphasize color and completeness.\" # Increased keywords and description length, added emphasis on color and completeness\n",
        "\n",
        "    return image_prompt\n",
        "\n",
        "# --- Generation Button ---\n",
        "if st.button(\"Generate Story and Images\"):\n",
        "    if tokenizer is not None and text_model is not None and image_pipe is not None:\n",
        "        with st.spinner(\"Generating your story and images...\"):\n",
        "            story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"]\n",
        "            full_story = \"\"\n",
        "            story_segments_for_images = []\n",
        "            previous_text = \"\" # Keep track of previous text for coherence\n",
        "\n",
        "            for i, part in enumerate(story_parts):\n",
        "                # Refined prompt for text generation\n",
        "                # We'll provide the overall prompt and the previous text to guide the generation\n",
        "                if part == \"Introduction\":\n",
        "                    part_prompt = f\"Write a {genre} story with a {tone} tone for {audience} about: {prompt}\\n\\n{part}:\"\n",
        "                else:\n",
        "                     # Provide more context for subsequent parts and explicitly ask for a continuation\n",
        "                     part_prompt = f\"{full_story.strip()}\\n\\nWrite the {part} of the story, continuing from the previous part:\"\n",
        "\n",
        "\n",
        "                inputs = tokenizer(part_prompt, return_tensors=\"pt\")\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
        "\n",
        "                # Adjust max_length for subsequent parts to avoid exceeding limits\n",
        "                # Also increase max_length slightly for Climax to encourage more text\n",
        "                current_max_length = len(tokenizer(part_prompt).input_ids) + length\n",
        "                if i > 0: # For Conflict, Climax, Resolution, adjust max length based on previous text\n",
        "                    current_max_length = len(tokenizer(full_story).input_ids) + length + (50 if part == \"Climax\" else 0) # Add extra tokens for Climax\n",
        "\n",
        "\n",
        "                generated_ids = text_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=current_max_length,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.85,\n",
        "                    top_k=50,\n",
        "                    top_p=0.92,\n",
        "                    repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                # Remove the input prompt from the generated text\n",
        "                # Use regex to remove the part prompt more reliably\n",
        "                generated_text = re.sub(re.escape(part_prompt), \"\", generated_text, 1).strip()\n",
        "\n",
        "\n",
        "                # Update previous_text for the next iteration\n",
        "                previous_text = generated_text\n",
        "\n",
        "\n",
        "                full_story += generated_text + \"\\\\n\\\\n\" # Add a newline for separation\n",
        "\n",
        "                # Generate a more descriptive image prompt\n",
        "                image_prompt = generate_image_prompt(generated_text.strip(), part, genre, tone)\n",
        "                story_segments_for_images.append({\"text\": generated_text.strip(), \"prompt_for_image\": image_prompt, \"image\": None}) # Add a placeholder for the generated image\n",
        "\n",
        "\n",
        "            st.subheader(\"Generated Story\")\n",
        "\n",
        "            # --- Display Story Segments and Generate Images ---\n",
        "            for i, segment in enumerate(story_segments_for_images):\n",
        "                st.markdown(f\"**{story_parts[i]}:**\")\n",
        "                st.write(segment[\"text\"])\n",
        "\n",
        "                # --- Image Generation ---\n",
        "                if segment.get(\"prompt_for_image\"):\n",
        "                    try:\n",
        "                        image = image_pipe(segment[\"prompt_for_image\"]).images[0]\n",
        "                        segment[\"image\"] = image # Store the generated image\n",
        "                        st.image(image, caption=f\"{story_parts[i]} Illustration\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error generating image for {story_parts[i]}: {e}\")\n",
        "                else:\n",
        "                    st.info(f\"No image prompt generated for {story_parts[i]}.\")\n",
        "\n",
        "\n",
        "            # --- Optional Export/Enhancements Section ---\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Options\")\n",
        "\n",
        "            # Store story data in session state for export\n",
        "            st.session_state['story_data'] = story_segments_for_images\n",
        "            st.session_state['full_story_text'] = full_story\n",
        "    else:\n",
        "        st.error(\"Model loading failed. Please check your model paths.\")\n",
        "\n",
        "\n",
        "# --- Export Functionality ---\n",
        "if 'story_data' in st.session_state and st.session_state['story_data']:\n",
        "    story_segments_for_images = st.session_state['story_data']\n",
        "    full_story_text = st.session_state['full_story_text']\n",
        "    story_parts = [\"Introduction\", \"Conflict\", \"Climax\", \"Resolution\"] # Define story_parts again for the export section\n",
        "\n",
        "    # Export as Text File\n",
        "    st.download_button(\n",
        "        label=\"Export as Text File\",\n",
        "        data=full_story_text,\n",
        "        file_name=\"story.txt\",\n",
        "        mime=\"text/plain\"\n",
        "    )\n",
        "\n",
        "    # Export as Image Gallery (ZIP file of images)\n",
        "    if any(segment[\"image\"] is not None for segment in story_segments_for_images):\n",
        "        with io.BytesIO() as archive:\n",
        "            with zipfile.ZipFile(archive, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for i, segment in enumerate(story_segments_for_images):\n",
        "                    if segment[\"image\"]:\n",
        "                        img_byte_arr = io.BytesIO()\n",
        "                        segment[\"image\"].save(img_byte_arr, format='PNG')\n",
        "                        zipf.writestr(f'scene_{i+1}_{story_parts[i].lower().replace(\" \", \"_\")}.png', img_byte_arr.getvalue())\n",
        "            st.download_button(\n",
        "                label=\"Export as Image Gallery (ZIP)\",\n",
        "                data=archive.getvalue(),\n",
        "                file_name=\"story_images.zip\",\n",
        "                mime=\"application/zip\"\n",
        "            )\n",
        "\n",
        "    # Placeholder for Export as PDF (requires additional libraries like reportlab)\n",
        "    # st.markdown(\"*(PDF export requires additional libraries and implementation)*\")\n",
        "\n",
        "    # Placeholder for Export as Storybook (requires custom formatting/HTML generation)\n",
        "    # st.markdown(\"*(Storybook export requires custom formatting and implementation)*\")\n",
        "\n",
        "# --- Team Attribution ---\n",
        "st.markdown(\"---\") # Add a separator line\n",
        "st.markdown(\"<p style='text-align: center;'>Team: The Glitch</p>\", unsafe_allow_html=True) # Centered attribution"
      ],
      "metadata": {
        "id": "7rSyR5_RS7qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create .streamlit directory if it doesn't exist\n",
        "if not os.path.exists('.streamlit'):\n",
        "    os.makedirs('.streamlit')\n",
        "\n",
        "# Write toml configuration file\n",
        "config_content = \"\"\"\n",
        "[server]\n",
        "enableCORS = true\n",
        "enableXsrfProtection = false\n",
        "\"\"\"\n",
        "with open('.streamlit/config.toml', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"Created .streamlit/config.toml with necessary configurations.\")"
      ],
      "metadata": {
        "id": "F4fmeQwzTGBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Terminate any active ngrok tunnels\n",
        "print(\"Terminating existing ngrok tunnels...\")\n",
        "ngrok.kill()\n",
        "\n",
        "# Set environment variable to potentially help with CUDA memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "print(\"Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n",
        "\n",
        "\n",
        "# Use the provided ngrok authtoken\n",
        "NGROK_AUTH_TOKEN = \"32MhBmqSbeLzF3l5sHA5XvXOdlj_67k12yrBbq6wkJHLTy8cv\" # Replace with your actual auth token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "\n",
        "# Define the Streamlit app file and port\n",
        "STREAMLIT_APP_FILE = \"guvi.py\"\n",
        "STREAMLIT_PORT = 8501\n",
        "\n",
        "# Function to run Streamlit in a separate thread\n",
        "def run_streamlit():\n",
        "    # Use `streamlit run` command to run the app\n",
        "    os.system(f\"streamlit run {STREAMLIT_APP_FILE} &>/dev/null\")\n",
        "\n",
        "# Start Streamlit in a thread\n",
        "print(f\"Starting Streamlit app '{STREAMLIT_APP_FILE}' on port {STREAMLIT_PORT}...\")\n",
        "streamlit_thread = threading.Thread(target=run_streamlit)\n",
        "streamlit_thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "streamlit_thread.start()\n",
        "\n",
        "# Wait a moment for Streamlit to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Open a tunnel to the Streamlit port\n",
        "try:\n",
        "    print(f\"Opening ngrok tunnel to port {STREAMLIT_PORT}...\")\n",
        "    public_url = ngrok.connect(STREAMLIT_PORT).public_url\n",
        "    print(f\"Streamlit app available at: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ngrok tunnel: {e}\")\n",
        "    print(\"Please ensure Streamlit is running and your ngrok authtoken is correct.\")"
      ],
      "metadata": {
        "id": "0I_xLKwATM8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HdxCNcRWYQnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b8a555"
      },
      "source": [
        "# Task\n",
        "Save the current notebook and the `guvi.py` file, then upload them to the GitHub repository at \"https://github.com/Shrshpathak/AI-Story-Generator-Hackathon.git\", ensuring the notebook renders correctly on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17342c8f"
      },
      "source": [
        "## Save notebook\n",
        "\n",
        "### Subtask:\n",
        "Save the current state of your Colab notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "623700d6"
      },
      "source": [
        "## Download files\n",
        "\n",
        "### Subtask:\n",
        "Download both the `.ipynb` notebook file and the `guvi.py` file from Colab.\n"
      ]
    }
  ]
}